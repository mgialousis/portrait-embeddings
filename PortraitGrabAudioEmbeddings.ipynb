{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "WINDOWS = True\n",
    "\n",
    "# Define the list (or tuple) of prefixes to be excluded\n",
    "exclude_prefixes = (\"12HSC\", \"1HCB2\", \"1SNB2\", \"E1LY2\", \"I12ME\", \"IG1V2\", \"V1PR2\", \"OU12L\", \"1H2LJ\")\n",
    "\n",
    "\n",
    "# Base directories\n",
    "if WINDOWS:\n",
    "    INPUT_DIR = 'W:/Portrait/Embeddings/Portrait Transcripts'\n",
    "else:\n",
    "    # For linux\n",
    "    INPUT_DIR = '/Volumes/mgialou/Portrait/Embeddings/Portrait Transcripts'\n",
    "\n",
    "OUTPUT_BASE_DIR = os.path.join(INPUT_DIR, 'audio_embeddings')\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_IDS       = [\n",
    "    \"openai/whisper-large-v3\",\n",
    "    \"utter-project/mHuBERT-147\",\n",
    "    \"facebook/mms-1b-all\"\n",
    "]\n",
    "# try to use the richer \"soundfile\" backend first\n",
    "try:\n",
    "    torchaudio.set_audio_backend(\"soundfile\")\n",
    "    print(\"üîä using torchaudio backend = soundfile\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è  couldn't switch to soundfile backend; staying with default\")\n",
    "\n",
    "# Original path prefix on Mac/Linux\n",
    "original_prefix = '/Users/miltos/Desktop/ftp_portrait/PORTRAIT/'\n",
    "\n",
    "# Windows-specific prefix (note the use of a raw string to handle backslashes)\n",
    "windows_prefix = r'P:\\1_ejecutando\\IN PORTRAIT\\INVESTIGACION\\3. Experimental phase\\Participantes\\Participantes_datos\\revisados\\buenos\\\\'\n",
    "\n"
   ],
   "id": "a9facd6e90eed12c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_and_resample_audio(audio_file: str,\n",
    "                            target_sample_rate: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loads an audio file and resamples it to target_sample_rate.\n",
    "    Falls back to soundfile.read if torchaudio.load fails.\n",
    "    Returns a 1D Tensor [num_samples].\n",
    "    \"\"\"\n",
    "    # strip file:// and whitespace\n",
    "    if audio_file.startswith(\"file://\"):\n",
    "        audio_file = audio_file[len(\"file://\"):]\n",
    "    audio_file = audio_file.strip()\n",
    "\n",
    "    if not os.path.isfile(audio_file):\n",
    "        raise FileNotFoundError(f\"Not found: {audio_file!r}\")\n",
    "\n",
    "    try:\n",
    "        # try torchaudio first\n",
    "        waveform, orig_sr = torchaudio.load(audio_file)  # -> Tensor[C,L]\n",
    "        # if multichannel, pick first channel\n",
    "        if waveform.ndim > 1:\n",
    "            waveform = waveform[0:1, :]\n",
    "    except Exception as e:\n",
    "        # fallback to soundfile\n",
    "        import soundfile as sf\n",
    "        data, orig_sr = sf.read(audio_file)            # -> ndarray [L] or [L,C]\n",
    "        if data.ndim > 1:\n",
    "            data = data[:, 0]                          # take first channel\n",
    "        waveform = torch.from_numpy(data).unsqueeze(0) # [1, L]\n",
    "\n",
    "    # resample if needed\n",
    "    if orig_sr != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(\n",
    "            orig_freq=orig_sr, new_freq=target_sample_rate\n",
    "        )\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # squeeze to 1D [L]\n",
    "    return waveform.squeeze(0)"
   ],
   "id": "e7844700536d751a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_audio_embeddings(audio_tensor: torch.Tensor,\n",
    "                             feature_extractor,\n",
    "                             model,\n",
    "                             device) -> tuple[np.ndarray, np.ndarray]:\n",
    "    inputs = feature_extractor(audio_tensor,\n",
    "                               sampling_rate=16000,\n",
    "                               return_tensors=\"pt\")\n",
    "    if \"input_features\" in inputs:\n",
    "        x = inputs[\"input_features\"].to(device)\n",
    "    elif \"input_values\" in inputs:\n",
    "        x = inputs[\"input_values\"].to(device)\n",
    "    else:\n",
    "        raise ValueError(\"No 'input_features' or 'input_values' in extractor output\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Whisper‚Äêstyle vs Wav2Vec2/HuBERT\n",
    "        if hasattr(model, \"encoder\") and x.ndim == 3:\n",
    "            h = model.encoder(x).last_hidden_state\n",
    "        else:\n",
    "            key = \"input_features\" if x.ndim == 3 else \"input_values\"\n",
    "            out = model(**{ key: x })\n",
    "            h = out.last_hidden_state\n",
    "\n",
    "    h = h.cpu().numpy()  # [B, T, D]\n",
    "    mean_emb = h.mean(axis=1).reshape(-1)\n",
    "    max_emb  = h.max(axis=1).reshape(-1)\n",
    "    return mean_emb, max_emb"
   ],
   "id": "aa3a92a5dea088d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_user_file(xlsx_path: str,\n",
    "                      output_dir: str,\n",
    "                      model_ids: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in xlsx_path:\n",
    "      - read 'audio_filepath'\n",
    "      - compute and save mean/max embeddings for each model\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "\n",
    "    # load device once\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load all models+extractors\n",
    "    models = {}\n",
    "    for model_id in model_ids:\n",
    "        print(f\"  Loading {model_id} ‚Ä¶\")\n",
    "        fe = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "        m  = AutoModel.from_pretrained(model_id).to(device).eval()\n",
    "        name = model_id.split(\"/\")[-1]\n",
    "        models[name] = (fe, m)\n",
    "        # per‚Äêmodel folder\n",
    "        os.makedirs(os.path.join(output_dir, name), exist_ok=True)\n",
    "\n",
    "    # iterate rows\n",
    "    for idx, row in df.iterrows():\n",
    "        path = row.get(\"audio_filepath\")\n",
    "        # Check if running on Windows\n",
    "        if WINDOWS:\n",
    "            # Replace the prefix if the file path begins with the original prefix\n",
    "            if path.startswith(original_prefix):\n",
    "                path = path.replace(original_prefix, windows_prefix)\n",
    "\n",
    "        if not isinstance(path, str) or not os.path.isfile(path):\n",
    "            print(f\"[row {idx}] skipping invalid path: {path!r}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            wav = load_and_resample_audio(path)\n",
    "        except Exception as e:\n",
    "            print(f\"[row {idx}] error loading {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        base = os.path.splitext(os.path.basename(path))[0]\n",
    "        for name, (fe, m) in models.items():\n",
    "            mean_emb, max_emb = compute_audio_embeddings(wav, fe, m, device)\n",
    "            out_dir = os.path.join(output_dir, name)\n",
    "            np.save(os.path.join(out_dir, f\"{base}-mean.npy\"), mean_emb)\n",
    "            np.save(os.path.join(out_dir, f\"{base}-max.npy\"), max_emb)\n",
    "            print(f\"[row {idx}] ‚Üí {name}: saved {base}-{{mean,max}}.npy\")\n"
   ],
   "id": "660445ada02f21bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t0 = time.perf_counter()\n",
    "for fn in os.listdir(INPUT_DIR):\n",
    "    if not fn.endswith(\"_transcripts_Model.xlsx\") or fn.startswith(\".\"):\n",
    "        continue\n",
    "    # Skip files whose name starts with any of the excluded prefixes\n",
    "    if fn.startswith(exclude_prefixes):\n",
    "        continue\n",
    "    user_id = fn.split(\"_\")[0]\n",
    "    in_path = os.path.join(INPUT_DIR, fn)\n",
    "    out_dir = os.path.join(OUTPUT_BASE_DIR, user_id)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f\"\\n== User {user_id} ({fn}) ==\")\n",
    "    process_user_file(in_path, out_dir, MODEL_IDS)\n",
    "\n",
    "print(f\"\\nAll done in {(time.perf_counter()-t0)/60:.2f} min.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1c743951b0043e49",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
