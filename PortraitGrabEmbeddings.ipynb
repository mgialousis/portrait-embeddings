{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:49.027399Z",
     "start_time": "2025-06-09T12:26:48.912768Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor\n",
    "\n",
    "# List of transformer models to use for text embeddings.\n",
    "MODELS_LIST = [\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    \"FacebookAI/roberta-large\",\n",
    "    \"bertin-project/bertin-roberta-base-spanish\",  # RoBERTa Spanish\n",
    "    \"dccuchile/bert-base-spanish-wwm-cased\",  # BETO\n",
    "    \"google-bert/bert-base-multilingual-cased\",  # multilingual BERT\n",
    "]\n",
    "\n",
    "# Base directories\n",
    "INPUT_DIR = '/Volumes/mgialou/Portrait/Embeddings/Portrait Transcripts'\n",
    "OUTPUT_BASE_DIR = os.path.join(INPUT_DIR, 'embeddings')\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:50.410591Z",
     "start_time": "2025-06-09T12:26:50.407505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string using tiktoken encoding.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return 0\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n"
   ],
   "id": "9093a208f2738fa",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:51.210042Z",
     "start_time": "2025-06-09T12:26:51.207120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_transformer_embedding(text: str, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Given a text string, tokenize it using the provided tokenizer and compute\n",
    "    its embedding using the provided model. The embedding is taken from the [CLS]\n",
    "    token (assumed to be the first token).\n",
    "    \"\"\"\n",
    "    # Tokenize text. Adjust truncation and padding as needed.\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Forward pass without gradient calculation.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the last hidden state and extract the [CLS] token representation.\n",
    "    hidden_state = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "    embedding = hidden_state[:, 0, :].reshape(-1)\n",
    "    return embedding"
   ],
   "id": "42ac0455917085e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:51.993669Z",
     "start_time": "2025-06-09T12:26:51.990399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_transformer_models(models_list, device):\n",
    "    \"\"\"\n",
    "    Load each transformer model and its tokenizer, move to the specified device.\n",
    "    Returns a dict mapping model_name -> (tokenizer, model, model_id).\n",
    "    \"\"\"\n",
    "    transformers_models = {}\n",
    "    for model_name in models_list:\n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        # derive a simple model identifier for filenames\n",
    "        model_id = model_name.split('/')[-1]\n",
    "        transformers_models[model_name] = (tokenizer, model, model_id)\n",
    "    return transformers_models"
   ],
   "id": "842bf71b5fdc24e0",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:52.390384Z",
     "start_time": "2025-06-09T12:26:52.386257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_user_file(xlsx_path, output_dir, transformers_models, encoding_name='cl100k_base'):\n",
    "    \"\"\"\n",
    "    Process a single user's Excel file of transcripts:\n",
    "      - Reads columns: questionnaire, question, transcript_whisper\n",
    "      - Generates embeddings per model\n",
    "      - Saves each embedding as <questionnaire>_<question>-<model_id>.npy\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.get('transcript_whisper', '') or ''\n",
    "        # build filename stem\n",
    "        questionnaire = str(row.get('questionnaire', ''))\n",
    "        question = str(row.get('question', ''))\n",
    "        file_stem = f\"{questionnaire}_{question}\"\n",
    "        # count tokens\n",
    "        token_count = num_tokens_from_string(text, encoding_name) if text else 0\n",
    "\n",
    "        for model_name, (tokenizer, model, model_id) in transformers_models.items():\n",
    "            if token_count == 0:\n",
    "                filename = f\"{file_stem}-{model_id}-EMPTY.npy\"\n",
    "                embedding = np.empty((0,))\n",
    "            else:\n",
    "                filename = f\"{file_stem}-{model_id}.npy\"\n",
    "                embedding = compute_transformer_embedding(text, tokenizer, model, torch.device(model.device.type))\n",
    "\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "            np.save(save_path, embedding)\n",
    "            print(f\"Saved embedding to {save_path}\")\n"
   ],
   "id": "82ed634979ffd061",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:27:10.980038Z",
     "start_time": "2025-06-09T12:26:52.700828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "# determine device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load models once\n",
    "transformers_models = load_transformer_models(MODELS_LIST, device)\n",
    "\n",
    "# iterate over all Excel files in INPUT_DIR\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    if not fname.endswith('_transcripts_Model.xlsx'):\n",
    "        continue\n",
    "    user_id = fname.split('_')[0]\n",
    "    user_input_path = os.path.join(INPUT_DIR, fname)\n",
    "    user_output_dir = os.path.join(OUTPUT_BASE_DIR, user_id)\n",
    "    os.makedirs(user_output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nProcessing user {user_id} from {user_input_path}...\")\n",
    "    process_user_file(user_input_path, user_output_dir, transformers_models)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"\\nAll users processed in {round((toc - tic)/60, 2)} min.\")"
   ],
   "id": "4f2442db71f3abc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen/Qwen3-Embedding-0.6B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/huggingface_hub/file_download.py:627\u001B[39m, in \u001B[36mxet_get\u001B[39m\u001B[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001B[39m\n\u001B[32m    625\u001B[39m     progress.update(progress_bytes)\n\u001B[32m--> \u001B[39m\u001B[32m627\u001B[39m download_files(\n\u001B[32m    628\u001B[39m     xet_download_info,\n\u001B[32m    629\u001B[39m     endpoint=connection_info.endpoint,\n\u001B[32m    630\u001B[39m     token_info=(connection_info.access_token, connection_info.expiration_unix_epoch),\n\u001B[32m    631\u001B[39m     token_refresher=token_refresher,\n\u001B[32m    632\u001B[39m     progress_updater=[progress_updater],\n\u001B[32m    633\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m device = torch.device(\u001B[33m'\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# load models once\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m transformers_models = load_transformer_models(MODELS_LIST, device)\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# iterate over all Excel files in INPUT_DIR\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m fname \u001B[38;5;129;01min\u001B[39;00m os.listdir(INPUT_DIR):\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mload_transformer_models\u001B[39m\u001B[34m(models_list, device)\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLoading model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m model = AutoModel.from_pretrained(model_name)\n\u001B[32m     11\u001B[39m model.to(device)\n\u001B[32m     12\u001B[39m model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    569\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    570\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m571\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class.from_pretrained(\n\u001B[32m    572\u001B[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001B[32m    573\u001B[39m     )\n\u001B[32m    574\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    575\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    576\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    577\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/transformers/modeling_utils.py:309\u001B[39m, in \u001B[36mrestore_default_torch_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    307\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m309\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    311\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/transformers/modeling_utils.py:4422\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4412\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   4413\u001B[39m     gguf_file\n\u001B[32m   4414\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4415\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m ((\u001B[38;5;28misinstance\u001B[39m(device_map, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mdisk\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map.values()) \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mdisk\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map)\n\u001B[32m   4416\u001B[39m ):\n\u001B[32m   4417\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   4418\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4419\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mloaded from GGUF files.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4420\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m4422\u001B[39m checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n\u001B[32m   4423\u001B[39m     pretrained_model_name_or_path=pretrained_model_name_or_path,\n\u001B[32m   4424\u001B[39m     subfolder=subfolder,\n\u001B[32m   4425\u001B[39m     variant=variant,\n\u001B[32m   4426\u001B[39m     gguf_file=gguf_file,\n\u001B[32m   4427\u001B[39m     from_tf=from_tf,\n\u001B[32m   4428\u001B[39m     from_flax=from_flax,\n\u001B[32m   4429\u001B[39m     use_safetensors=use_safetensors,\n\u001B[32m   4430\u001B[39m     cache_dir=cache_dir,\n\u001B[32m   4431\u001B[39m     force_download=force_download,\n\u001B[32m   4432\u001B[39m     proxies=proxies,\n\u001B[32m   4433\u001B[39m     local_files_only=local_files_only,\n\u001B[32m   4434\u001B[39m     token=token,\n\u001B[32m   4435\u001B[39m     user_agent=user_agent,\n\u001B[32m   4436\u001B[39m     revision=revision,\n\u001B[32m   4437\u001B[39m     commit_hash=commit_hash,\n\u001B[32m   4438\u001B[39m     transformers_explicit_filename=transformers_explicit_filename,\n\u001B[32m   4439\u001B[39m )\n\u001B[32m   4441\u001B[39m is_sharded = sharded_metadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4442\u001B[39m is_quantized = hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/transformers/modeling_utils.py:1024\u001B[39m, in \u001B[36m_get_resolved_checkpoint_files\u001B[39m\u001B[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, transformers_explicit_filename)\u001B[39m\n\u001B[32m   1009\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1010\u001B[39m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[32m   1011\u001B[39m     cached_file_kwargs = {\n\u001B[32m   1012\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mcache_dir\u001B[39m\u001B[33m\"\u001B[39m: cache_dir,\n\u001B[32m   1013\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mforce_download\u001B[39m\u001B[33m\"\u001B[39m: force_download,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m_commit_hash\u001B[39m\u001B[33m\"\u001B[39m: commit_hash,\n\u001B[32m   1023\u001B[39m     }\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n\u001B[32m   1026\u001B[39m     \u001B[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001B[39;00m\n\u001B[32m   1027\u001B[39m     \u001B[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001B[39;00m\n\u001B[32m   1028\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m resolved_archive_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001B[32m   1029\u001B[39m         \u001B[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/transformers/utils/hub.py:312\u001B[39m, in \u001B[36mcached_file\u001B[39m\u001B[34m(path_or_repo_id, filename, **kwargs)\u001B[39m\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcached_file\u001B[39m(\n\u001B[32m    255\u001B[39m     path_or_repo_id: Union[\u001B[38;5;28mstr\u001B[39m, os.PathLike],\n\u001B[32m    256\u001B[39m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m    257\u001B[39m     **kwargs,\n\u001B[32m    258\u001B[39m ) -> Optional[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m    259\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    260\u001B[39m \u001B[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n\u001B[32m    261\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    310\u001B[39m \u001B[33;03m    ```\u001B[39;00m\n\u001B[32m    311\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m312\u001B[39m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n\u001B[32m    313\u001B[39m     file = file[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n\u001B[32m    314\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/transformers/utils/hub.py:470\u001B[39m, in \u001B[36mcached_files\u001B[39m\u001B[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    467\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    468\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) == \u001B[32m1\u001B[39m:\n\u001B[32m    469\u001B[39m         \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m470\u001B[39m         hf_hub_download(\n\u001B[32m    471\u001B[39m             path_or_repo_id,\n\u001B[32m    472\u001B[39m             filenames[\u001B[32m0\u001B[39m],\n\u001B[32m    473\u001B[39m             subfolder=\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) == \u001B[32m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n\u001B[32m    474\u001B[39m             repo_type=repo_type,\n\u001B[32m    475\u001B[39m             revision=revision,\n\u001B[32m    476\u001B[39m             cache_dir=cache_dir,\n\u001B[32m    477\u001B[39m             user_agent=user_agent,\n\u001B[32m    478\u001B[39m             force_download=force_download,\n\u001B[32m    479\u001B[39m             proxies=proxies,\n\u001B[32m    480\u001B[39m             resume_download=resume_download,\n\u001B[32m    481\u001B[39m             token=token,\n\u001B[32m    482\u001B[39m             local_files_only=local_files_only,\n\u001B[32m    483\u001B[39m         )\n\u001B[32m    484\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    485\u001B[39m         snapshot_download(\n\u001B[32m    486\u001B[39m             path_or_repo_id,\n\u001B[32m    487\u001B[39m             allow_patterns=full_filenames,\n\u001B[32m   (...)\u001B[39m\u001B[32m    496\u001B[39m             local_files_only=local_files_only,\n\u001B[32m    497\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fn(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/huggingface_hub/file_download.py:1008\u001B[39m, in \u001B[36mhf_hub_download\u001B[39m\u001B[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[39m\n\u001B[32m    988\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[32m    989\u001B[39m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[32m    990\u001B[39m         local_dir=local_dir,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1005\u001B[39m         local_files_only=local_files_only,\n\u001B[32m   1006\u001B[39m     )\n\u001B[32m   1007\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_cache_dir(\n\u001B[32m   1009\u001B[39m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[32m   1010\u001B[39m         cache_dir=cache_dir,\n\u001B[32m   1011\u001B[39m         \u001B[38;5;66;03m# File info\u001B[39;00m\n\u001B[32m   1012\u001B[39m         repo_id=repo_id,\n\u001B[32m   1013\u001B[39m         filename=filename,\n\u001B[32m   1014\u001B[39m         repo_type=repo_type,\n\u001B[32m   1015\u001B[39m         revision=revision,\n\u001B[32m   1016\u001B[39m         \u001B[38;5;66;03m# HTTP info\u001B[39;00m\n\u001B[32m   1017\u001B[39m         endpoint=endpoint,\n\u001B[32m   1018\u001B[39m         etag_timeout=etag_timeout,\n\u001B[32m   1019\u001B[39m         headers=hf_headers,\n\u001B[32m   1020\u001B[39m         proxies=proxies,\n\u001B[32m   1021\u001B[39m         token=token,\n\u001B[32m   1022\u001B[39m         \u001B[38;5;66;03m# Additional options\u001B[39;00m\n\u001B[32m   1023\u001B[39m         local_files_only=local_files_only,\n\u001B[32m   1024\u001B[39m         force_download=force_download,\n\u001B[32m   1025\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/huggingface_hub/file_download.py:1161\u001B[39m, in \u001B[36m_hf_hub_download_to_cache_dir\u001B[39m\u001B[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[39m\n\u001B[32m   1158\u001B[39m \u001B[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001B[39;00m\n\u001B[32m   1160\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m WeakFileLock(lock_path):\n\u001B[32m-> \u001B[39m\u001B[32m1161\u001B[39m     _download_to_tmp_and_move(\n\u001B[32m   1162\u001B[39m         incomplete_path=Path(blob_path + \u001B[33m\"\u001B[39m\u001B[33m.incomplete\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   1163\u001B[39m         destination_path=Path(blob_path),\n\u001B[32m   1164\u001B[39m         url_to_download=url_to_download,\n\u001B[32m   1165\u001B[39m         proxies=proxies,\n\u001B[32m   1166\u001B[39m         headers=headers,\n\u001B[32m   1167\u001B[39m         expected_size=expected_size,\n\u001B[32m   1168\u001B[39m         filename=filename,\n\u001B[32m   1169\u001B[39m         force_download=force_download,\n\u001B[32m   1170\u001B[39m         etag=etag,\n\u001B[32m   1171\u001B[39m         xet_file_data=xet_file_data,\n\u001B[32m   1172\u001B[39m     )\n\u001B[32m   1173\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(pointer_path):\n\u001B[32m   1174\u001B[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/huggingface_hub/file_download.py:1710\u001B[39m, in \u001B[36m_download_to_tmp_and_move\u001B[39m\u001B[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001B[39m\n\u001B[32m   1708\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m xet_file_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m is_xet_available():\n\u001B[32m   1709\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1710\u001B[39m     xet_get(\n\u001B[32m   1711\u001B[39m         incomplete_path=incomplete_path,\n\u001B[32m   1712\u001B[39m         xet_file_data=xet_file_data,\n\u001B[32m   1713\u001B[39m         headers=headers,\n\u001B[32m   1714\u001B[39m         expected_size=expected_size,\n\u001B[32m   1715\u001B[39m         displayed_filename=filename,\n\u001B[32m   1716\u001B[39m     )\n\u001B[32m   1717\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1718\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m xet_file_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/huggingface_hub/file_download.py:622\u001B[39m, in \u001B[36mxet_get\u001B[39m\u001B[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001B[39m\n\u001B[32m    611\u001B[39m     displayed_filename = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdisplayed_filename[:\u001B[32m40\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m(â€¦)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    613\u001B[39m progress_cm = _get_progress_bar_context(\n\u001B[32m    614\u001B[39m     desc=displayed_filename,\n\u001B[32m    615\u001B[39m     log_level=logger.getEffectiveLevel(),\n\u001B[32m   (...)\u001B[39m\u001B[32m    619\u001B[39m     _tqdm_bar=_tqdm_bar,\n\u001B[32m    620\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m622\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m progress_cm \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[32m    624\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprogress_updater\u001B[39m(progress_bytes: \u001B[38;5;28mfloat\u001B[39m):\n\u001B[32m    625\u001B[39m         progress.update(progress_bytes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Embeddings/lib/python3.12/site-packages/tqdm/std.py:1138\u001B[39m, in \u001B[36mtqdm.__exit__\u001B[39m\u001B[34m(self, exc_type, exc_value, traceback)\u001B[39m\n\u001B[32m   1135\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m   1136\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1138\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, exc_type, exc_value, traceback):\n\u001B[32m   1139\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1140\u001B[39m         \u001B[38;5;28mself\u001B[39m.close()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
