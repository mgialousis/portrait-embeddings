{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken\n",
    "import warnings\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from typing import Dict, Optional, List, Union\n",
    "\n",
    "# Suppress HF hub symlink warnings (optional)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*cache-system uses symlinks by default.*\",\n",
    ")\n",
    "\n",
    "WINDOWS = True\n",
    "# Define the list (or tuple) of prefixes to be excluded\n",
    "exclude_prefixes = (\"12HSC\", \"1HCB2\", \"1SNB2\", \"E1LY2\", \"I12ME\", \"IG1V2\", \"V1PR2\", \"OU12L\", \"1H2LJ\", \"1BUR2\", \"1BMX2\", \"1U2TL\", \"D1D2V\", \"W1CB2\",\n",
    "                   \"12UFZ\", \"ENF12\",  \n",
    "                   \"1Q2YF\", \"EGJ12\", \"HZP12\",\"I1E2H\", \"JN12W\", \"NN12O\", \n",
    "                   )\n",
    "\n",
    "\n",
    "# List of transformer models to use for text embeddings.\n",
    "MODELS_LIST = [\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    \"FacebookAI/roberta-large\",\n",
    "    \"bertin-project/bertin-roberta-base-spanish\",  # RoBERTa Spanish\n",
    "    \"dccuchile/bert-base-spanish-wwm-cased\",  # BETO\n",
    "    \"google-bert/bert-base-multilingual-cased\",  # multilingual BERT\n",
    "]\n",
    "\n",
    "# Base directories\n",
    "if WINDOWS:\n",
    "    INPUT_DIR = 'W:/Portrait/Embeddings/Portrait Transcripts'\n",
    "else:\n",
    "    # For linux\n",
    "    INPUT_DIR = '/Volumes/mgialou/Portrait/Embeddings/Portrait Transcripts'\n",
    "\n",
    "OUTPUT_BASE_DIR = os.path.join(INPUT_DIR, 'embeddings')\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912e62f-a8ec-4e57-9b3e-9014963f0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Embedding():\n",
    "    def __init__(self, model_name_or_path, instruction=None,  use_fp16: bool = True, use_cuda: bool = True, max_length=8192):\n",
    "        if instruction is None:\n",
    "            instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        self.instruction = instruction\n",
    "        if is_flash_attn_2_available() and use_cuda:\n",
    "            self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "        if use_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side='left')\n",
    "        self.max_length=max_length\n",
    "    \n",
    "    def last_token_pool(self, last_hidden_states: Tensor,\n",
    "        attention_mask: Tensor) -> Tensor:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "    def get_detailed_instruct(self, task_description: str, query: str) -> str:\n",
    "        if task_description is None:\n",
    "            task_description = self.instruction\n",
    "        return f'Instruct: {task_description}\\nQuery:{query}'\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], str], is_query: bool = False, instruction=None, dim: int = -1):\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "        if is_query:\n",
    "            sentences = [self.get_detailed_instruct(instruction, sent) for sent in sentences]\n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        inputs.to(self.model.device)\n",
    "        model_outputs = self.model(**inputs)\n",
    "        output = self.last_token_pool(model_outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        if dim != -1:\n",
    "            output = output[:, :dim]\n",
    "        output  = F.normalize(output, p=2, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093a208f2738fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string using tiktoken encoding.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return 0\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac0455917085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformer_embedding(text: str, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Given a text string, tokenize it using the provided tokenizer and compute\n",
    "    its embedding using the provided model. The embedding is taken from the [CLS]\n",
    "    token (assumed to be the first token).\n",
    "    \"\"\"\n",
    "    # Tokenize text. Adjust truncation and padding as needed.\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Forward pass without gradient calculation.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the last hidden state and extract the [CLS] token representation.\n",
    "    hidden_state = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "    embedding = hidden_state[:, 0, :].reshape(-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bf71b5fdc24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformer_models(models_list, device):\n",
    "    \"\"\"\n",
    "    Load each transformer model and its tokenizer, move to the specified device.\n",
    "    Returns a dict mapping model_name -> (tokenizer, model, model_id).\n",
    "    \"\"\"\n",
    "    transformers_models = {}\n",
    "    for model_name in models_list:\n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        # derive a simple model identifier for filenames\n",
    "        model_id = model_name.split('/')[-1]\n",
    "        # special-case Qwen\n",
    "        if model_name.lower().startswith(\"qwen/qwen3-embedding\"):\n",
    "            print(f\"Loading Qwen3Embedding for {model_name}\")\n",
    "            loader = Qwen3Embedding(\n",
    "                model_name_or_path=model_name,\n",
    "                use_fp16=True,\n",
    "                use_cuda=torch.cuda.is_available(),\n",
    "            )\n",
    "            transformers_models[model_name] = (\"qwen\", loader, model_id)\n",
    "            continue\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        transformers_models[model_name] = (tokenizer, model, model_id)\n",
    "    return transformers_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed634979ffd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_file(xlsx_path, output_dir, transformers_models, encoding_name='cl100k_base'):\n",
    "    \"\"\"\n",
    "    Process a single user's Excel file of transcripts:\n",
    "      - Reads columns: questionnaire, question, transcript_whisper\n",
    "      - Generates embeddings per model\n",
    "      - Saves each embedding as <questionnaire>_<question>-<model_id>.npy\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.get('transcript_whisper', '') or ''\n",
    "        # build filename stem\n",
    "        questionnaire = str(row.get('questionnaire', ''))\n",
    "        question = str(row.get('question', ''))\n",
    "        file_stem = f\"{questionnaire}_{question}\"\n",
    "        # count tokens\n",
    "        token_count = 0\n",
    "        if text and text != \"-\":\n",
    "            token_count = num_tokens_from_string(text, encoding_name)\n",
    "\n",
    "        for model_name, (tokenizer, model, model_id) in transformers_models.items():\n",
    "            # create per-model folder under user\n",
    "            model_dir = os.path.join(user_output_dir, model_id)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            if token_count == 0:\n",
    "                print(f\"\\nSKATA empty\\n\")\n",
    "                embedding = np.empty((0,))\n",
    "                filename = f\"{file_stem}_EMPTY.npy\"\n",
    "\n",
    "            else:\n",
    "                filename = f\"{file_stem}.npy\"\n",
    "                if tokenizer == \"qwen\":\n",
    "                    with torch.no_grad():\n",
    "                        # Qwen3Embedding.encode will return a torch.Tensor\n",
    "                        embedding = model.encode(text, is_query=False).cpu().numpy().reshape(-1)\n",
    "                else:\n",
    "                    embedding = compute_transformer_embedding(text, tokenizer, model, torch.device(model.device.type))\n",
    "\n",
    "            save_path = os.path.join(model_dir, filename)\n",
    "            np.save(save_path, embedding)\n",
    "            print(f\"Saved embedding to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eece8cb0d1e915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_check(excel_files):\n",
    "    # Second loop: Iterate over the collected list of Excel filenames and check each file for having exactly 42 rows\n",
    "    warnings = []\n",
    "\n",
    "    for file_path in excel_files:\n",
    "        df = pd.read_excel(file_path)\n",
    "        row_count = df.shape[0]  # Number of rows in the DataFrame\n",
    "        if row_count != 41:\n",
    "            warnings.append(f\"File '{file_path}' has {row_count} rows (expected 42).\")\n",
    "\n",
    "    # If there are any warnings, print them and optionally stop further processing.\n",
    "    if warnings:\n",
    "        print(\"WARNING: The following files do not have exactly 42 rows:\")\n",
    "        for warning in warnings:\n",
    "            print(warning)\n",
    "        raise ValueError(\"Pre-check failed: Not all files have 42 rows.\")\n",
    "    else:\n",
    "        print(\"Pre-check passed: All files have exactly 42 rows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2442db71f3abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "# determine device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load models once\n",
    "transformers_models = load_transformer_models(MODELS_LIST, device)\n",
    "\n",
    "# First loop: Iterate over all files in INPUT_DIR and collect the Excel files that meet the criteria\n",
    "excel_files = []\n",
    "\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    # Skip hidden files\n",
    "    if fname.startswith('.'):\n",
    "        continue\n",
    "    # Check the file name ends with the expected suffix\n",
    "    if not fname.lower().endswith('_transcripts_model.xlsx'):\n",
    "        continue\n",
    "    # Skip files whose name starts with any of the excluded prefixes\n",
    "    if fname.startswith(exclude_prefixes):\n",
    "        continue\n",
    "    print(\"Add \" + fname)\n",
    "    # Add the full file path to the list\n",
    "    excel_files.append(os.path.join(INPUT_DIR, fname))\n",
    "    \n",
    "row_check(excel_files)\n",
    "\n",
    "# iterate over all Excel files in INPUT_DIR\n",
    "for fname in excel_files:\n",
    "    user_id = fname.split('_')[0][-5:]\n",
    "    user_input_path = os.path.join(INPUT_DIR, fname)\n",
    "    user_output_dir = os.path.join(OUTPUT_BASE_DIR, user_id)\n",
    "    os.makedirs(user_output_dir, exist_ok=True)\n",
    "    print(f\"\\nProcessing user {user_id} from {user_input_path} and output: {user_output_dir} and {os.path.join(OUTPUT_BASE_DIR, user_id)}  ...\")\n",
    "    process_user_file(user_input_path, user_output_dir, transformers_models)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"\\nAll users processed in {round((toc - tic)/60, 2)} min.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
