{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:49.027399Z",
     "start_time": "2025-06-09T12:26:48.912768Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken\n",
    "import warnings\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from typing import Dict, Optional, List, Union\n",
    "\n",
    "# Suppress HF hub symlink warnings (optional)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*cache-system uses symlinks by default.*\",\n",
    ")\n",
    "\n",
    "# List of transformer models to use for text embeddings.\n",
    "MODELS_LIST = [\n",
    "    \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    \"FacebookAI/roberta-large\",\n",
    "    \"bertin-project/bertin-roberta-base-spanish\",  # RoBERTa Spanish\n",
    "    \"dccuchile/bert-base-spanish-wwm-cased\",  # BETO\n",
    "    \"google-bert/bert-base-multilingual-cased\",  # multilingual BERT\n",
    "]\n",
    "\n",
    "# Base directories\n",
    "INPUT_DIR = 'W:/Portrait/Embeddings/Portrait Transcripts'\n",
    "# For linux\n",
    "#INPUT_DIR = '/Volumes/mgialou/Portrait/Embeddings/Portrait Transcripts'\n",
    "\n",
    "OUTPUT_BASE_DIR = os.path.join(INPUT_DIR, 'embeddings')\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9912e62f-a8ec-4e57-9b3e-9014963f0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Embedding():\n",
    "    def __init__(self, model_name_or_path, instruction=None,  use_fp16: bool = True, use_cuda: bool = True, max_length=8192):\n",
    "        if instruction is None:\n",
    "            instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        self.instruction = instruction\n",
    "        if is_flash_attn_2_available() and use_cuda:\n",
    "            self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "        if use_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side='left')\n",
    "        self.max_length=max_length\n",
    "    \n",
    "    def last_token_pool(self, last_hidden_states: Tensor,\n",
    "        attention_mask: Tensor) -> Tensor:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "    def get_detailed_instruct(self, task_description: str, query: str) -> str:\n",
    "        if task_description is None:\n",
    "            task_description = self.instruction\n",
    "        return f'Instruct: {task_description}\\nQuery:{query}'\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], str], is_query: bool = False, instruction=None, dim: int = -1):\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "        if is_query:\n",
    "            sentences = [self.get_detailed_instruct(instruction, sent) for sent in sentences]\n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        inputs.to(self.model.device)\n",
    "        model_outputs = self.model(**inputs)\n",
    "        output = self.last_token_pool(model_outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        if dim != -1:\n",
    "            output = output[:, :dim]\n",
    "        output  = F.normalize(output, p=2, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9093a208f2738fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:50.410591Z",
     "start_time": "2025-06-09T12:26:50.407505Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string using tiktoken encoding.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return 0\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42ac0455917085e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:51.210042Z",
     "start_time": "2025-06-09T12:26:51.207120Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_transformer_embedding(text: str, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Given a text string, tokenize it using the provided tokenizer and compute\n",
    "    its embedding using the provided model. The embedding is taken from the [CLS]\n",
    "    token (assumed to be the first token).\n",
    "    \"\"\"\n",
    "    # Tokenize text. Adjust truncation and padding as needed.\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Forward pass without gradient calculation.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the last hidden state and extract the [CLS] token representation.\n",
    "    hidden_state = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "    embedding = hidden_state[:, 0, :].reshape(-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "842bf71b5fdc24e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:51.993669Z",
     "start_time": "2025-06-09T12:26:51.990399Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_transformer_models(models_list, device):\n",
    "    \"\"\"\n",
    "    Load each transformer model and its tokenizer, move to the specified device.\n",
    "    Returns a dict mapping model_name -> (tokenizer, model, model_id).\n",
    "    \"\"\"\n",
    "    transformers_models = {}\n",
    "    for model_name in models_list:\n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        # derive a simple model identifier for filenames\n",
    "        model_id = model_name.split('/')[-1]\n",
    "        # special-case Qwen\n",
    "        if model_name.lower().startswith(\"qwen/qwen3-embedding\"):\n",
    "            print(f\"Loading Qwen3Embedding for {model_name}\")\n",
    "            loader = Qwen3Embedding(\n",
    "                model_name_or_path=model_name,\n",
    "                use_fp16=True,\n",
    "                use_cuda=torch.cuda.is_available(),\n",
    "            )\n",
    "            transformers_models[model_name] = (\"qwen\", loader, model_id)\n",
    "            continue\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        transformers_models[model_name] = (tokenizer, model, model_id)\n",
    "    return transformers_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82ed634979ffd061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:26:52.390384Z",
     "start_time": "2025-06-09T12:26:52.386257Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_user_file(xlsx_path, output_dir, transformers_models, encoding_name='cl100k_base'):\n",
    "    \"\"\"\n",
    "    Process a single user's Excel file of transcripts:\n",
    "      - Reads columns: questionnaire, question, transcript_whisper\n",
    "      - Generates embeddings per model\n",
    "      - Saves each embedding as <questionnaire>_<question>-<model_id>.npy\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "    for _, row in df.iterrows():\n",
    "        text = row.get('transcript_whisper', '') or ''\n",
    "        # build filename stem\n",
    "        questionnaire = str(row.get('questionnaire', ''))\n",
    "        question = str(row.get('question', ''))\n",
    "        file_stem = f\"{questionnaire}_{question}\"\n",
    "        # count tokens\n",
    "        token_count = num_tokens_from_string(text, encoding_name) if text else 0\n",
    "\n",
    "        for model_name, (tokenizer, model, model_id) in transformers_models.items():\n",
    "            # create per-model folder under user\n",
    "            model_dir = os.path.join(user_output_dir, model_id)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "            filename = f\"{file_stem}.npy\"\n",
    "            if token_count == 0:\n",
    "                print(f\"\\nSKATA empty\\n\")\n",
    "                embedding = np.empty((0,))\n",
    "            else:\n",
    "                if tokenizer == \"qwen\":\n",
    "                    with torch.no_grad():\n",
    "                        # Qwen3Embedding.encode will return a torch.Tensor\n",
    "                        embedding = model.encode(text, is_query=False).cpu().numpy().reshape(-1)\n",
    "                else:\n",
    "                    embedding = compute_transformer_embedding(text, tokenizer, model, torch.device(model.device.type))\n",
    "\n",
    "            save_path = os.path.join(model_dir, filename)\n",
    "            np.save(save_path, embedding)\n",
    "            print(f\"Saved embedding to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2442db71f3abc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:27:10.980038Z",
     "start_time": "2025-06-09T12:26:52.700828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Qwen/Qwen3-Embedding-0.6B...\n",
      "Loading Qwen3Embedding for Qwen/Qwen3-Embedding-0.6B\n",
      "Loading model FacebookAI/roberta-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model bertin-project/bertin-roberta-base-spanish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at bertin-project/bertin-roberta-base-spanish and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model dccuchile/bert-base-spanish-wwm-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model google-bert/bert-base-multilingual-cased...\n",
      "\n",
      "Processing user 12HSC from W:/Portrait/Embeddings/Portrait Transcripts\\12HSC_transcripts_Model.xlsx...\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\Qwen3-Embedding-0.6B\\GR_Survey_q1.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\roberta-large\\GR_Survey_q1.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bertin-roberta-base-spanish\\GR_Survey_q1.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-spanish-wwm-cased\\GR_Survey_q1.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-multilingual-cased\\GR_Survey_q1.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\Qwen3-Embedding-0.6B\\GR_Survey_q2.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\roberta-large\\GR_Survey_q2.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bertin-roberta-base-spanish\\GR_Survey_q2.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-spanish-wwm-cased\\GR_Survey_q2.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-multilingual-cased\\GR_Survey_q2.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\Qwen3-Embedding-0.6B\\GR_Survey_q3.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\roberta-large\\GR_Survey_q3.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bertin-roberta-base-spanish\\GR_Survey_q3.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-spanish-wwm-cased\\GR_Survey_q3.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-multilingual-cased\\GR_Survey_q3.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\Qwen3-Embedding-0.6B\\GR_Survey_q4.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\roberta-large\\GR_Survey_q4.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bertin-roberta-base-spanish\\GR_Survey_q4.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-spanish-wwm-cased\\GR_Survey_q4.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-multilingual-cased\\GR_Survey_q4.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\Qwen3-Embedding-0.6B\\GR_Survey_q5.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\roberta-large\\GR_Survey_q5.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bertin-roberta-base-spanish\\GR_Survey_q5.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-spanish-wwm-cased\\GR_Survey_q5.npy\n",
      "Saved embedding to W:/Portrait/Embeddings/Portrait Transcripts\\embeddings\\12HSC\\bert-base-multilingual-cased\\GR_Survey_q5.npy\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "# determine device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load models once\n",
    "transformers_models = load_transformer_models(MODELS_LIST, device)\n",
    "\n",
    "# iterate over all Excel files in INPUT_DIR\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    # Skip hidden files\n",
    "    if fname.startswith('.'):\n",
    "        continue\n",
    "    if not fname.endswith('_transcripts_Model.xlsx'):\n",
    "        continue\n",
    "    user_id = fname.split('_')[0]\n",
    "    user_input_path = os.path.join(INPUT_DIR, fname)\n",
    "    user_output_dir = os.path.join(OUTPUT_BASE_DIR, user_id)\n",
    "    os.makedirs(user_output_dir, exist_ok=True)\n",
    "    print(f\"\\nProcessing user {user_id} from {user_input_path}...\")\n",
    "    process_user_file(user_input_path, user_output_dir, transformers_models)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(f\"\\nAll users processed in {round((toc - tic)/60, 2)} min.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
