{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9b45b-4a7b-4055-a131-c8fa5e27f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor\n",
    "\n",
    "# List of transformer models to use for text embeddings.\n",
    "\n",
    "options_text = [\n",
    "    \"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    \"distilbert/distilroberta-base\",\n",
    "    \"j-hartmann/emotion-english-roberta-large\",\n",
    "    \"FacebookAI/roberta-large\",\n",
    "    \"bertin-project/bertin-roberta-base-spanish\",  # RoBERTa Spanish\n",
    "    \"dccuchile/bert-base-spanish-wwm-cased\",  # BETO\n",
    "    \"google-bert/bert-base-multilingual-cased\",  # multilingual BERT\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb534aea-f789-41eb-8929-ea7d12fb3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string using tiktoken encoding.\n",
    "    \"\"\"\n",
    "    if not string:\n",
    "        return 0\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d05b3c-f75c-4bb6-893e-b2c5c62ff7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformer_embedding(text: str, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Given a text string, tokenize it using the provided tokenizer and compute\n",
    "    its embedding using the provided model. The embedding is taken from the [CLS]\n",
    "    token (assumed to be the first token).\n",
    "    \"\"\"\n",
    "    # Tokenize text. Adjust truncation and padding as needed.\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Forward pass without gradient calculation.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the last hidden state and extract the [CLS] token representation.\n",
    "    hidden_state = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "    embedding = hidden_state[:, 0, :].reshape(-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f4970-232b-453c-8b1d-af1345601828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xlsx_transformers(xlsx_file, models_list, encoding_name=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Processes an Excel file with columns: id, subject, question, transcript.\n",
    "    For each row, it:\n",
    "      - Counts tokens in the transcript.\n",
    "      - Uses transformer models to calculate a text embedding.\n",
    "      - Saves the embedding as a .npy file in a separate folder.\n",
    "\n",
    "    The output filename is of the form:\n",
    "       <file_id>-<model_identifier>.npy\n",
    "    where <model_identifier> is derived from the model name.\n",
    "    \"\"\"\n",
    "    # Read the Excel file.\n",
    "    df = pd.read_excel(xlsx_file)\n",
    "\n",
    "    # Determine the device to use for model inference.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load each model and its tokenizer once.\n",
    "    transformers_models = {}\n",
    "    output_folders = {}\n",
    "    for model_name in models_list:\n",
    "        print(f\"Loading model {model_name} ...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        transformers_models[model_name] = (tokenizer, model)\n",
    "        # Use the part after the slash as model identifier (or the full name if not available).\n",
    "        model_identifier = model_name.split('/')[1] if '/' in model_name else model_name\n",
    "        # Prepare output folder.\n",
    "        folder_name = f\"transformer_embeddings_{model_identifier}\"\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        output_folders[model_name] = folder_name\n",
    "\n",
    "\n",
    "    # Process each row in the Excel file.\n",
    "    for _, row in df.iterrows():\n",
    "        # Remove any extension from the file id.\n",
    "        file_id = os.path.splitext(row['id'])[0]\n",
    "        transcript = row['transcript']\n",
    "        if pd.isna(transcript) or transcript == \"\":\n",
    "            transcript = \"\"\n",
    "            print(\"SKATA: transcript is empty\")\n",
    "        else:\n",
    "            transcript = transcript.replace(\"\\n\", \" \")\n",
    "\n",
    "        # Count tokens (useful for logging or ensuring transcript length limitations).\n",
    "        token_count = num_tokens_from_string(transcript, encoding_name) if transcript else 0\n",
    "\n",
    "        # Process the transcript for each transformer model.\n",
    "        for model_name, (tokenizer, model) in transformers_models.items():\n",
    "            model_identifier = model_name.split('/')[1] if '/' in model_name else model_name\n",
    "            output_folder = output_folders[model_name]\n",
    "\n",
    "            if token_count == 0:\n",
    "                output_filename = f\"{file_id}-{model_identifier}-EMPTY.npy\"\n",
    "                embedding = np.empty((0,))\n",
    "            else:\n",
    "                output_filename = f\"{file_id}-{model_identifier}.npy\"\n",
    "                embedding = compute_transformer_embedding(transcript, tokenizer, model, device)\n",
    "\n",
    "            output_filepath = os.path.join(output_folder, output_filename)\n",
    "            with open(output_filepath, 'wb') as f:\n",
    "                np.save(f, embedding)\n",
    "            print(f\"Saved embedding to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebf5b8-8c0a-4e07-b3cf-559c362bb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = \"transcriptions.xlsx\"  # Path to your Excel file.\n",
    "tic = time.perf_counter()\n",
    "process_xlsx_transformers(excel_path, options_text)\n",
    "toc = time.perf_counter()\n",
    "print('Duration:', round((toc - tic) / 60, 2), 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49c26b-111f-495b-ab09-1fcfbc061cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
